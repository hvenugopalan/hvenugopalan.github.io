<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Building Segmentation</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Krub:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Bikin - v2.0.0
  * Template URL: https://bootstrapmade.com/bikin-free-simple-landing-page-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center">

      <nav class="nav-menu d-none d-lg-block">
        <ul>
          <li><a href="#home">Home</a></li>
          <li><a href="#pd">Problem definition</a></li>
          <li><a href="#m">Motivation</a></li>
          <li><a href="#rw">Related Work</a></li>
          <li><a href="#ds">Dataset</a></li>
          <li><a href="#method">Methodology</a></li>
          <li><a href="#te">Testing and Evaluation</a></li>
          <li><a href="#r">Results</a></li>
          <li><a href="#c">Challenges</a></li>
          <li><a href="#r">References</a></li>


        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex align-items-center">

    <div class="container d-flex flex-column align-items-center justify-content-center" data-aos="fade-up">
      <h1></h1>
      <br />
      <br />
      <br />
      <h1>Segmenting Buildings for Disaster Resilience</h1>
      <!-- <img src="assets/img/cv_background2.jpg" class="img-fluid hero-img" alt="" data-aos="zoom-in" data-aos-delay="150"> -->
    </div>

  </section><!-- End Hero -->

  <main id="main">
    <!-- ======= Features Section ======= -->
    <section id="pd" class="features" data-aos="fade-up">
      <div class="container">

        <div class="section-title">
          <h2>Problem Definition</h2>
        </div>
        <div>
          <p>Our project will use computer vision for segmenting building footprints from aerial imagery. The data
            consists of images taken by a drone from 10 different regions across Africa and
            has been provided through the <a
              href="https://www.drivendata.org/competitions/60/building-segmentation-disaster-resilience/page/150/">Open
              Cities AI Challenge</a>.
            Our goal is to classify the presence or absence of a building on a pixel-by-pixel basis. </p>
        </div>

        <!-- <div class="row content">
          <div class="col-md-5" data-aos="fade-right" data-aos-delay="100">
            <img src="assets/img/features-1.png" class="img-fluid" alt="">
          </div>
          <div class="col-md-7 pt-4" data-aos="fade-left" data-aos-delay="100">
            <h3>Voluptatem dignissimos provident quasi corporis voluptates sit assumenda.</h3>
            <p class="font-italic">
              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et
              dolore
              magna aliqua.
            </p>
            <ul>
              <li><i class="icofont-check"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat.</li>
              <li><i class="icofont-check"></i> Duis aute irure dolor in reprehenderit in voluptate velit.</li>
              <li><i class="icofont-check"></i> Ullam est qui quos consequatur eos accusamus.</li>
            </ul>
          </div>
        </div>

        <div class="row content">
          <div class="col-md-5 order-1 order-md-2" data-aos="fade-left">
            <img src="assets/img/features-2.png" class="img-fluid" alt="">
          </div>
          <div class="col-md-7 pt-5 order-2 order-md-1" data-aos="fade-right">
            <h3>Corporis temporibus maiores provident</h3>
            <p class="font-italic">
              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et
              dolore
              magna aliqua.
            </p>
            <p>
              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in
              voluptate
              velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
              culpa qui officia deserunt mollit anim id est laborum
            </p>
          </div>
        </div>

        <div class="row content">
          <div class="col-md-5" data-aos="fade-right">
            <img src="assets/img/features-3.png" class="img-fluid" alt="">
          </div>
          <div class="col-md-7 pt-5" data-aos="fade-left">
            <h3>Sunt consequatur ad ut est nulla consectetur reiciendis animi voluptas</h3>
            <p>Cupiditate placeat cupiditate placeat est ipsam culpa. Delectus quia minima quod. Sunt saepe odit aut
              quia voluptatem hic voluptas dolor doloremque.</p>
            <ul>
              <li><i class="icofont-check"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat.</li>
              <li><i class="icofont-check"></i> Duis aute irure dolor in reprehenderit in voluptate velit.</li>
              <li><i class="icofont-check"></i> Facilis ut et voluptatem aperiam. Autem soluta ad fugiat.</li>
            </ul>
          </div>
        </div>

        <div class="row content">
          <div class="col-md-5 order-1 order-md-2" data-aos="fade-left">
            <img src="assets/img/features-4.png" class="img-fluid" alt="">
          </div>
          <div class="col-md-7 pt-5 order-2 order-md-1" data-aos="fade-right">
            <h3>Quas et necessitatibus eaque impedit ipsum animi consequatur incidunt in</h3>
            <p class="font-italic">
              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et
              dolore
              magna aliqua.
            </p>
            <p>
              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in
              voluptate
              velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
              culpa qui officia deserunt mollit anim id est laborum
            </p>
          </div>
        </div>

      </div> -->
    </section><!-- End Features Section -->

    <section id="m" class="features" data-aos="fade-up">
      <div class="container">

        <div class="section-title">
          <h2>Motivation</h2>
        </div>
        <div>
          <p>With the significant paradigm shift in the construction
            industry and advancement in construction technology, the
            typical construction timeline has decreased drastically. One
            challenge for cities is managing the risk of disasters in a
            constantly changing environment. Buildings, roads, and
            other critical infrastructure need to be mapped frequently
            and accurately to represent assets important to every community. Hence, the key to disaster risk management
            (DRM)
            is knowing the location of the resources vulnerable to damage or disruption by natural hazards.</p>
        </div>
      </div>
    </section>

    <!--Related Works-->
    <section id="rw" class="steps" data-aos="fade-up">
      <div class="container">
        <div class="section-title">
          <h2>Related Work</h2>
        </div>
        <div class="row no-gutters" data-aos="fade-up">

          <div class="col-md-6 content-item" data-aos="fade-up" data-aos-delay="100">
            <p>There is a large amount of literature on semantic segmentation. O Ronneberger et al. architectured a
              convolutional
              neural network (CNN) for medical image segmentation that
              consists of contracting and expanding paths. [7]. The main
              difference in the architecture is the concatenation of the
              contractive path feature maps to the corresponding expansion path layer. The model achieves good
              performance
              even
              on very few annotated images.</p><br>
            <p>Alex Kendall, Vijay Badrinarayana and Roberto Cipolla
              proposed SegNet[3], a CNN for multi-class pixel-wise segmentation. The model architecture consists of
              encoder, decoder
              and a pixel-wise classifier. SegNet achieves great performance in terms of memory and time during
              inference.
            </p><br>
            <p>In 2018, Zongwei Zhou et. al redesigned the skip pathways in U-Net architecture to reduce the semantic
              gap
              between the encoder and the decoder, and provided reasoning
              for this design. U-Net++ has shown to perform better than
              U-Net across multiple medical segmentation tasks.</p>
            <!-- <img src="../assets/img/tiff.jpg" class="img-fluid"><br/>
            <img src="../assets/img/geojson.jpg" class="img-fluid"> -->

          </div>

          <div class="col-md-6 content-item" data-aos="fade-up" data-aos-delay="200">

            <p>Researchers from ETH Zurich studied the trade-off between pixel accuracy and readily available but less
              accurate
              reference data for image segmentation by using online map
              data from OpenStreetMap (OSM) to automatically obtain
              weakly labeled training data for three classes, namely buildings, roads, and background (everything other
              than buildings and roads).[5]. They use deep learning techniques,
              which they purport as data hungry and robust to label noise.</p><br>
            <p>Sebastien Ohleyer [6] performed segmentation on aerial
              image datasets using a Mask R-CNN and compared its performance to baseline results obtained with two more
              classical networks: a Fully Convolutional Network (FCN) and
              its extension using a Multi-Layer Perceptron (MLP). It was
              found that Mask R-CNN achieves better perfomance than
              MLP and FCN but needed a lot of hyperparameter tuning
              to predict well on very dense cities, ie. where the buildings
              were really close.</p><br>
            <p>Our work deals with the segmentation of remotely
              sensed images provided by OpenAI using the existing deep
              learning models like U-Net.</p>
            </p>
          </div>
        </div>
    </section>

    <!--Dataset-->
    <section id="ds" class="features" data-aos="fade-up">
      <div class="container">
        <div class="section-title">
          <h2>Dataset</h2>
        </div>
        <div>
          <p>The data collected using OpenStreetMap (OSM) is
            stored in the form of SpatioTemporal Asset Catalogs
            (STACs). A STAC is a standardized specification that allows easy query of geospatial imagery and labels.
            Precisely, the images in the dataset are stored as large Cloud
            Optimized GeoTiffs (COG) and the building footprints are
            stored in GeoJSON format (GeoJSON). A series of JSON
            files that comprise a STAC reference each other as well as
            the geospatial assets (e.g. imagery, labels). The data is divided into training data and test data, as
            explained further.</p>
        </div>
      </div>

      <section class="steps">
        <div class="container">
          <div class="row no-gutters" data-aos="fade-up">

            <div class="col-md-6 content-item" data-aos="fade-up" data-aos-delay="100">
              <h4>Training data</h4>
              <p>We have used Tier 1 of the dataset given by the Open
                Cities AI Challenge for training the building segmentation
                model. The data is divided into training and validation sets
                from different drone images (in cloud-optimized GeoTIFF
                format) and its accompanying ground-truth labels of manually traced building outlines (in GeoJSON
                format). The
                below table depicts a summary of the training data.</p>

                <table class="training">
                  <thead>
                    <tr>
                      <th>City</th>
                      <th>Scene Count</th>
                      <th>Building Count</th>
                    </tr>
                    </thead>
                  <tbody>
                  <tr style="height: 21.951px;">
                  <td style="height: 21.951px;">Accra</td>
                  <td style="height: 21.951px;">4</td>
                  <td style="height: 21.951px;">33585</td>
                  </tr>
                  <tr style="height: 21px;">
                  <td style="height: 21px;">Dar es Salaam</td>
                  <td style="height: 21px;">6</td>
                  <td style="height: 21px;">1231171</td>
                  </tr>
                  <tr style="height: 21px;">
                  <td style="height: 21px;">Kampala</td>
                  <td style="height: 21px;">1</td>
                  <td style="height: 21px;">4056</td>
                  </tr>
                  <tr style="height: 21px;">
                  <td style="height: 21px;">Montepuez</td>
                  <td style="height: 21px;">4</td>
                  <td style="height: 21px;">6947</td>
                  </tr>
                  <tr style="height: 21px;">
                  <td style="height: 21px;">Niamey</td>
                  <td style="height: 21px;">1</td>
                  <td style="height: 21px;">634</td>
                  </tr>
                  <tr style="height: 21px;">
                  <td style="height: 21px;">Pointe-Noire</td>
                  <td style="height: 21px;">2</td>
                  <td style="height: 21px;">8731</td>
                  </tr>
                  <tr style="height: 21px;">
                  <td style="height: 21px;">Zanzibar</td>
                  <td style="height: 21px;">13</td>
                  <td style="height: 21px;">13407</td>
                  </tr>
                  </tbody>
                  </table>
            </div>

            <div class="col-md-6 content-item" data-aos="fade-up" data-aos-delay="200">
              <h4>Test data</h4>
              <p>The test set consists of 11,481 1024 x 1024 pixel CloudOptimized GeoTIFF ”chips” derived from myriad of
                scenes.
                These scenes are mutually exclusive of the training set.
                Some of the test scenes are from regions that are present
                in the training set while others are not.</p>
            </div>
          </div>
        </div>
      </section>
    </section>

    <!-- ======= Methodology ======= -->
    <section id="method" class="steps">
      <div class="section-title">
        <h2>Methodology</h2>
        <h3>Data preprocessing</h3>
      </div>

      <div class="container">

        <div class="row no-gutters" data-aos="fade-up">

          <div class="col-lg-4 col-md-6 content-item" data-aos="fade-up" data-aos-delay="100">
            <span>01</span>
            <h4>Data visualization before preprocessing</h4>
            <p>The dataset is in the form of different folders, each
              named after different African cities. Each such
              folder in turn contains a specific number of ’scene’
              sub-folders and their corresponding ’label’ subfolders. For further steps, each ’scene’ sub-folder
              and its corresponding label sub-folder are considered as a pair. A few of the ’scene’ and their
              corresponding ’label’ images were visualized for better
              understanding of the structure of the given data, as shown in the images below. </p><br />
            <!-- <img src="../assets/img/tiff.jpg" class="img-fluid"><br/>
            <img src="../assets/img/geojson.jpg" class="img-fluid"> -->

          </div>

          <div class="col-lg-4 col-md-6 content-item" data-aos="fade-up" data-aos-delay="200">
            <span>02</span>
            <h4>Creation of masks for the tiles</h4>
            <p>In this step, the mask for each tile was created
              by cropping and using the solaris package. The
              3-channel mask has building footprint as the first
              channel, the boundary as the second and a 0-
              initialised third channel.
            </p>
          </div>

          <div class="col-lg-4 col-md-6 content-item" data-aos="fade-up" data-aos-delay="300">
            <span>03</span>
            <h4>Division of images into tiles</h4>
            <p>In this step, the python package ’supermercado’
              was used to generate square polygons representing
              the slippy map tiles (a term referring to modern web
              maps that allows zooming and panning around) out
              of the ’scene’ and ’label’ images. Slippy map tiles
              of tile size=256 and zoom level=19 were generated,
              which yields a manageable number of tiles and satisfactory segmentation results without much preprocessing
              or model training time.
              Then, a GeoDataFrame that consists of all the
              supermercado-generated slippy map tile polygons
              was generated using the Python library, ’geopandas’.
              Finally, all the tiles( created for all the cities and
              its corresponding labels) were merged into a single folder for training the building segmentation
              model.</p>
          </div>

          <div class="col-lg-4 col-md-6 content-item" data-aos="fade-up" data-aos-delay="100">
            <span>04</span>
            <h4>Division of tiles into training and validation sets</h4>
            <p>To improve the prediction efficiency of the building segmentation model, the given dataset was divided
              into training and validation sets in the ratio of
              75:25.</p>
          </div>

          <div class="col-lg-4 col-md-6 content-item" data-aos="fade-up" data-aos-delay="200">
            <span>05</span>
            <h4>Visualization of the preprocessed data</h4>
            <p>Few of the training set tiles were visualised to view
              the preprocessed data, as shown in the figure below.
              Then, the images and masks were saved in images
              and masks folder respectively</p>
          </div>

        </div>
    </section>
    <!-- ======= Preprocessing Images ======= -->
    <section class="services">
      <div class="container" data-aos="fade-up">

        <div class="row">
          <div class="col-md-6  d-flex align-items-stretch mb-5 mb-lg-0" data-aos="fade-up" data-aos-delay="100">
            <div class="icon-box">
              <img src="../assets/img/tiff.jpg" class="img-fluid">
              <h4 class="title"><a href="">Image scene example</a></h4>
            </div>
          </div>


          <div class="col-md-6  d-flex align-items-stretch mb-5 mb-lg-0" data-aos="fade-up" data-aos-delay="200">
            <div class="icon-box">
              <img src="../assets/img/geojson.jpg" class="img-fluid">
              <h4 class="title">Label Image example</h4>
            </div>
          </div>
        </div>
        <br>
        <br>
        <div class="row">

          <div class="col-md-2"></div>
          <div class="col-md-10 d-flex align-items-stretch mb-5 mb-lg-0" data-aos="fade-up" data-aos-delay="300">
            <div class="icon-box">
              <img src="../assets/img/tiles.jpg" class="img-fluid">
              <h4 class="title">Pre-processed Data Tile (Left: Image
                Tile, Right: Label Tile)</h4>
            </div>
          </div>
          <div class="col-md-2"></div>



        </div>

      </div>
    </section>
    <br>
    <br>

    <!--Training-->
    <section class="steps">
      <div class="section-title">
        <h3>Training</h3>
      </div>
      <div class="container">

        <div class="row no-gutters" data-aos="fade-up">

          <div class="col-md-6 content-item" data-aos="fade-up" data-aos-delay="100">
            <span>01</span>
            <h4>Setting up the data</h4>
            <p>The training dataset of tile images and masks created in the data pre-processing step was set up to
              load correctly into fastai for training and validation. With the fastai library,image segmentation
              models could be built with a few lines of code using
              the classes and methods provided. Transformations
              were added to the images and a databunch was created from the resulting augmented data to be used
              by the model learner.
            </p><br />
            <!-- <img src="../assets/img/tiff.jpg" class="img-fluid"><br/>
              <img src="../assets/img/geojson.jpg" class="img-fluid"> -->

          </div>

          <div class="col-md-6 content-item" data-aos="fade-up" data-aos-delay="200">
            <span>02</span>
            <h4>Define custom losses and metrics</h4>
            <p>Two loss functions that show high performance
              for image segmentation tasks, namely Jaccard
              Loss and Focal Loss were implemented. These
              loss functions were then combined and calculated
              across the 3 channels with adjustable weighting. In
              considering the image-wide context with adjustable
              weighing of the 3 target mask channels, the approach of combining a Jaccard loss with individual
              pixel-focused Binary Cross Entropy or Focal loss
              has been shown to consistently exceed the performance of single loss functions. Finally, our model
              evaluation metrics (accuracy and Jaccard score)
              were adapted to calculate a mean score for all the
              channels or by a specified individual channel.
            </p>
          </div>

          <div class="col-md-12 content-item" data-aos="fade-up" data-aos-delay="300">
            <span>03</span>
            <h4>Building the U-Net Model</h4>
            <p>A U-Net is a convolutional neural network that consists of two paths, namely the contraction path (the
              encoder) and the expansion path (the decoder), as
              shown in Figure 7. The encoder extracts features of
              3
              the image using convolutional and pooling layers.
              During the process of encoding, the size of the feature map gets reduced. The decoder then uses
              Upconvolution layers to recover the size of the feature
              map for segmentation of the image. Since the decoding process loses some of the higher level features
              learned by the encoder, the skip connections
              of the U-Net passes the outputs of the encoding layers directly to the decoding layers so that all the
              important pieces of information can be preserved [8].
              The fastai library was used to create and train our
              U-Net model with the U-Net learner class. The
              data along with the ImageNet pretrained encodernetwork (Resnet34), our accuracy function as well
              as a weight-decay of 0.01 was passed. Then, using
              the fastai function, an appropriate learning rate was
              found and the model training was started. First, we
              fine-tuned only the decoder part of the U-Net leaving the weights for the resnet34 encoder frozen for
              some epochs. Then, we unfroze all the trainable
              weights/layers of our model and trained for some
              more epochs. We also tracked the loss function
              metrics per epoch as training progressed to make
              sure the model continued to improve and did not
              over-fit the training data. Further, we generated
              some batches of predictions on the validation set,
              calculated and reshaped the image-wise loss values,
              and sorted them by highest loss first to see the worst
              performing results.
            </p>
          </div>



        </div>

      </div>
    </section>
  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">



    <div class="container d-md-flex py-4">

      <div class="mr-md-auto text-center text-md-left">
        <!-- <div class="copyright">
          &copy; Copyright <strong><span>Bikin</span></strong>. All Rights Reserved -->
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/bikin-free-simple-landing-page-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>